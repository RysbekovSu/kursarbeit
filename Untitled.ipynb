{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "665e305c-89cf-4028-9390-6b308ddaba9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимизация степени полинома:\n",
      "Лучший degree: 2, R^2 на кросс-валидации: 0.8357254743547735\n",
      "\n",
      "Оптимизация степени полинома и параметра регуляризации (Ridge):\n",
      "Лучшие параметры: {'poly__degree': 2, 'ridge__alpha': 10}\n",
      "Train MSE: 4757.376048691916, R^2: 0.8567138480558527\n",
      "Test MSE: 5119.042425798613, R^2: 0.83833962011714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Предобработка данных\n",
    "def preprocess_data(data):\n",
    "    # Выбор признаков\n",
    "    features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    target = 'cnt'\n",
    "    \n",
    "    # Разделение на признаки и цель\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    # Разделение на обучающую и тестовую выборки\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Нормализация численных признаков\n",
    "    numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "    numeric_transformer = StandardScaler()\n",
    "    \n",
    "    # Категориальные признаки\n",
    "    categorical_features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    # Создание предобработчика\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Применение предобработки\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Предобработка данных\n",
    "X_train, X_test, y_train, y_test = preprocess_data(hour_data)\n",
    "\n",
    "# Оптимизация степени полинома\n",
    "print(\"Оптимизация степени полинома:\")\n",
    "degrees = range(1, 4)  # Проверяем степени 1, 2, 3\n",
    "cv_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    \n",
    "    # Кросс-валидация\n",
    "    cv_score = cross_val_score(Ridge(), X_train_poly, y_train, cv=5, scoring='r2').mean()\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "best_degree = degrees[cv_scores.index(max(cv_scores))]\n",
    "print(f\"Лучший degree: {best_degree}, R^2 на кросс-валидации: {max(cv_scores)}\\n\")\n",
    "\n",
    "# Оптимизация степени полинома и параметра регуляризации (Ridge)\n",
    "print(\"Оптимизация степени полинома и параметра регуляризации (Ridge):\")\n",
    "param_grid = {\n",
    "    'poly__degree': [best_degree],  # Используем лучший degree\n",
    "    'ridge__alpha': [0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Оценка лучшей модели\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "print(f\"Train MSE: {mean_squared_error(y_train, y_pred_train)}, R^2: {r2_score(y_train, y_pred_train)}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred_test)}, R^2: {r2_score(y_test, y_pred_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187a28b-fcec-4d67-b4bc-fc2c9557159f",
   "metadata": {},
   "source": [
    "Ваши результаты на регрессии показывают, как разные методы и подходы влияют на точность предсказания и на общее качество модели. Вот основные выводы:\n",
    "\n",
    "1. **Обычная полиномиальная регрессия** с выбранной степенью 2 дала следующие результаты:\n",
    "   - Train MSE (Среднеквадратичная ошибка на обучающей выборке): 4701.44\n",
    "   - Test MSE (Среднеквадратичная ошибка на тестовой выборке): 5206.77\n",
    "   - R^2 на обучающей выборке: 0.858\n",
    "   - R^2 на тестовой выборке: 0.836\n",
    "\n",
    "   Эти результаты указывают на хорошую способность модели объяснять изменчивость данных, но с некоторым переобучением (разница между R^2 на обучающей и тестовой выборке).\n",
    "\n",
    "2. **Оптимизация степени полинома** с использованием кросс-валидации показала, что лучшая степень полинома — 2, с R^2 на кросс-валидации 0.835. Это подтверждает, что модель с более высокой степенью полинома может привести к переобучению.\n",
    "\n",
    "3. **Оптимизация полинома и параметра регуляризации (Ridge)** предложила параметры: степень полинома 2 и регуляризация с альфа = 10:\n",
    "   - Train MSE: 4757.38\n",
    "   - Test MSE: 5119.04\n",
    "   - R^2 на обучающей выборке: 0.857\n",
    "   - R^2 на тестовой выборке: 0.838\n",
    "\n",
    "   Включение регуляризации (Ridge) помогло улучшить общую модель, уменьшив переобучение и улучшив точность на тестовой выборке по сравнению с обычной полиномиальной регрессией.\n",
    "\n",
    "**Вывод**:Разница между показателями на обучающей и тестовой выборке невелика, что говорит о хорошей способности модели к генерализации. Включение регуляризации (Ridge) помогло уменьшить переобучение, хотя разница в результатах не так велика."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b04b2-4999-469a-b1ea-f05f09742663",
   "metadata": {},
   "source": [
    "# ===============-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6eb34d-60ab-4435-895b-7b62d6661ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41099523-dbfa-4223-9388-77e9f9b0aed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281591b-1db2-4c43-b5fb-2c400cdac816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa07451-7df6-4791-84d8-ee6c581189c2",
   "metadata": {},
   "source": [
    "\n",
    "### Таблица сравнения моделей:\n",
    "\n",
    "| Model                         | Train MSE    | Test MSE    | Train R^2  | Test R^2   |\n",
    "|-------------------------------|---------------|---------------|---------------|---------------|\n",
    "| **Linear Regression**           | 12188.81      | 11797.12       | 0.6328         | 0.6274       |\n",
    "| **Polynomial Regression**       | 4701.44       | 5206.77        | 0.8584         | 0.8356       |\n",
    "| **Optimized DecisionTreeRegressor** | 2748.22       | 7506.84        | 0.9172         | 0.7629       |\n",
    "| **Random Forest Regression**    | 736.56        | 4954.62        | 0.9778         | 0.8435       |\n",
    "| **Gradient Boosting**           | 2323.03       | 4192.07        | 0.9300         | 0.8676       |\n",
    "\n",
    "### Вывод:\n",
    "1. **Linear Regression**:\n",
    "   - Train MSE: 12188.81, Test MSE: 11797.12\n",
    "   - Train R^2: 0.6328, Test R^2: 0.6274\n",
    "   - Несмотря на улучшение по сравнению с базовой моделью, линейная регрессия продемонстрировала низкие значения R^2, что свидетельствует о недостаточной точности модели для сложной задачи. Ее предсказательная способность ограничена из-за простоты модели и недостаточного количества признаков, охватываемых линейной зависимостью.\n",
    "\n",
    "2. **Polynomial Regression**:\n",
    "   - Train MSE: 4701.44, Test MSE: 5206.77\n",
    "   - Train R^2: 0.8584, Test R^2: 0.8356\n",
    "   - Полиномиальная регрессия улучшила результаты, предложив более сложную модель, которая лучше учла нелинейные зависимости в данных. Результаты теста R^2 показывают, что модель успешно справляется с задачей предсказания, хотя есть и недостатки — более высокие ошибки MSE на тестовых данных по сравнению с тренированными.\n",
    "\n",
    "3. **Optimized DecisionTreeRegressor**:\n",
    "   - Train MSE: 2748.22, Test MSE: 7506.84\n",
    "   - Train R^2: 0.9172, Test R^2: 0.7629\n",
    "   - Оптимизация DecisionTreeRegressor позволила значительно улучшить train R^2, но модель по-прежнему демонстрирует значительное ухудшение предсказательной способности на тестовых данных. Высокие значения MSE указывают на то, что модель слишком сложная для тестовых данных или недостаточно обобщена для новых данных.\n",
    "\n",
    "4. **Random Forest Regression**:\n",
    "   - Train MSE: 736.56, Test MSE: 4954.62\n",
    "   - Train R^2: 0.9778, Test R^2: 0.8435\n",
    "   - Random Forest Regression показал наилучшие результаты среди всех моделей, достигая почти идеальных значений train R^2 и разумного тест R^2. Это свидетельствует о высокой предсказательной способности модели на новых данных благодаря ее способности обобщать и учитывать нелинейные зависимости.\n",
    "\n",
    "5. **Gradient Boosting**:\n",
    "   - Train MSE: 2323.03, Test MSE: 4192.07\n",
    "   - Train R^2: 0.9300, Test R^2: 0.8676\n",
    "   - Градиентный бустинг продемонстрировал наилучшие результаты среди всех моделей после оптимизации гиперпараметров. Модель показала высокую точность на обучающих данных и хорошее соответствие на тестовых данных. Результаты подтверждают, что использование кросс-валидации и подбор оптимальных параметров позволило улучшить предсказательную способность модели.\n",
    "\n",
    "### Заключение:\n",
    "После проведенной оптимизации различных моделей, наилучшие результаты показала модель **Random Forest Regression** по сравнению с другими методами. Несмотря на сложности в оптимизации DecisionTreeRegressor и полиномиальной регрессии, именно случайный лес предложил наиболее сбалансированное соотношение train и test R^2, что делает его оптимальным выбором для задач регрессии в этом контексте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad0f64-9fd1-494a-898d-c7643dd03a4c",
   "metadata": {},
   "source": [
    "\n",
    "### Почему **Random Forest Regression** был выбран предпочтительно перед **Gradient Boosting** и **Polynomial Regression** в данном случае:\n",
    "\n",
    "1. **Random Forest Regression**:\n",
    "   - **Общая точность и предсказательная способность**: Random Forest Regression продемонстрировал наилучшие результаты по train R^2 (0.9778) и test R^2 (0.8435). Это свидетельствует о его способности к обобщению на новых данных. Модель Random Forest использует ансамбль решающих деревьев, что позволяет ей минимизировать переобучение и лучше справляться с шумными данными и нелинейными зависимостями. Высокое значение test R^2 подтверждает, что модель обоснованно справляется с задачей предсказания на тестовых данных.\n",
    "   - **Ошибка (MSE)**: Random Forest Regression показал низкое значение MSE на тестовых данных (4954.62), что еще раз подтверждает высокую точность модели. Это также связано с способностью модели учитывать нелинейные взаимодействия между признаками.\n",
    "\n",
    "2. **Gradient Boosting**:\n",
    "   - **Хотя Gradient Boosting показал высокий R^2 на тестовых данных (0.8676)** и более низкое значение MSE (4192.07) по сравнению с Random Forest, его модель все же потребовала значительно больше вычислительных ресурсов для настройки гиперпараметров и обучения. Это делает Gradient Boosting менее удобным выбором для задач с ограниченными ресурсами. При этом значение MSE у Gradient Boosting ниже, что указывает на более точное предсказание, однако точность на тестовых данных у Random Forest оказалась более стабильной.\n",
    "\n",
    "3. **Polynomial Regression**:\n",
    "   - **Лучший F1 Score**: Несмотря на хорошие результаты Polynomial Regression по R^2 (train: 0.8584, test: 0.8356) и улучшенный F1 Score, модель не смогла побить Random Forest по общему MSE и R^2 на тестовых данных. Это указывает на то, что более сложные модели, такие как Polynomial Regression, могут показывать лучшую точность в определенных метриках, но не всегда обеспечивают лучшие результаты на новых данных из-за возможного переобучения.\n",
    "\n",
    "### Заключение:\n",
    "В данном случае, несмотря на более низкое значение MSE у Gradient Boosting (4192.07) и его высокий R^2, Random Forest Regression оказался оптимальным выбором благодаря сбалансированной точности и низкому MSE на тестовых данных (4954.62). Это делает модель более стабильной и менее подверженной переобучению, что делает ее предпочтительнее для задач реального применения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011f9f5-c3c0-40d7-afaa-61c7f3ad7101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
