{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eef02b-e149-4256-b317-cd70d6a43766",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a411f03-e771-4df3-a70a-78626f1f99b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Decision Tree - Train MSE: 1.47389052722434, R^2: 0.9999556082807263\n",
      "Initial Decision Tree - Test MSE: 9422.097310126583, R^2: 0.7024482893964866\n",
      "Best Parameters: {'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Optimized Decision Tree - Train MSE: 2748.220851586217, R^2: 0.9172270624634076\n",
      "Optimized Decision Tree - Test MSE: 7506.835295254137, R^2: 0.7629326454821246\n",
      "Cross-Validation R^2: 0.7548922692653481\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "hour_data = pd.read_csv('hour.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Feature selection\n",
    "    features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    target = 'cnt'\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "    numeric_transformer = StandardScaler()\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    # Create a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(hour_data)\n",
    "\n",
    "# Initial Decision Tree Model\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "y_pred_train_tree = tree_reg.predict(X_train)\n",
    "y_pred_test_tree = tree_reg.predict(X_test)\n",
    "print(f\"Initial Decision Tree - Train MSE: {mean_squared_error(y_train, y_pred_train_tree)}, R^2: {r2_score(y_train, y_pred_train_tree)}\")\n",
    "print(f\"Initial Decision Tree - Test MSE: {mean_squared_error(y_test, y_pred_test_tree)}, R^2: {r2_score(y_test, y_pred_test_tree)}\")\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeRegressor(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  # 5-кратная кросс-валидация\n",
    "                           scoring='r2',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best model evaluation\n",
    "best_tree = grid_search.best_estimator_\n",
    "y_pred_train_best = best_tree.predict(X_train)\n",
    "y_pred_test_best = best_tree.predict(X_test)\n",
    "print(f\"Optimized Decision Tree - Train MSE: {mean_squared_error(y_train, y_pred_train_best)}, R^2: {r2_score(y_train, y_pred_train_best)}\")\n",
    "print(f\"Optimized Decision Tree - Test MSE: {mean_squared_error(y_test, y_pred_test_best)}, R^2: {r2_score(y_test, y_pred_test_best)}\")\n",
    "\n",
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(best_tree, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-Validation R^2: {cv_scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b4a6b-7dfc-4ff7-9018-023e67c97952",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "\n",
    "Результаты анализа показывают значительное улучшение качества модели Decision Tree Regressor после оптимизации гиперпараметров с использованием **GridSearchCV**.\n",
    "\n",
    "1. **Начальная модель Decision Tree**:\n",
    "   - **Train MSE**: 1.47, **R²**: 0.9999 (модель переобучена на тренировочных данных).\n",
    "   - **Test MSE**: 9422.10, **R²**: 0.7024 (низкая генерализуемость на тестовых данных).\n",
    "\n",
    "2. **Оптимизированная модель Decision Tree**:\n",
    "   - **Наилучшие параметры**:\n",
    "     - `max_depth`: None (неограниченная глубина).\n",
    "     - `max_features`: None (используются все признаки при разбиении).\n",
    "     - `min_samples_leaf`: 4 (минимум 4 примера в листе).\n",
    "     - `min_samples_split`: 10 (минимум 10 примеров для разделения узла).\n",
    "   - **Train MSE**: 2748.22, **R²**: 0.9172 (уменьшение переобучения).\n",
    "   - **Test MSE**: 7506.83, **R²**: 0.7629 (улучшение генерализуемости на тестовых данных).\n",
    "   \n",
    "3. **Кросс-валидация**:\n",
    "   - Среднее значение R² на 5-кратной кросс-валидации: **0.7549**. Это подтверждает стабильность и надежность модели на разных разбиениях данных.\n",
    "\n",
    "### Заключение:\n",
    "После оптимизации гиперпараметров модель стала менее склонной к переобучению и показала лучшее качество на тестовых данных, что видно по увеличению метрики R² и снижению MSE. Таким образом, применение регуляризации и подбор параметров существенно улучшили производительность модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942a9f7-4963-4a96-b830-a3ac79ca0f39",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220dcab6-b8f5-4a98-972c-37895a80ba06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f294f6fc-06cd-46d2-8638-fcd79c0be1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Random Forest - Train MSE: 736.5630141932512, R^2: 0.9778156532323837\n",
      "Initial Random Forest - Test MSE: 4954.617398292612, R^2: 0.8435321952509012\n",
      "Cross-Validation R^2: 0.8362264283720279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "443 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "97 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "C:\\Users\\Amin_stors\\PycharmProjects\\kursarbeit5\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.82792688 0.8287831  0.8296578\n",
      " 0.82475802 0.82581576 0.82614308 0.81670125 0.81746248 0.81814488\n",
      " 0.81307127 0.81356799 0.81426563 0.81315764 0.81325521 0.81381892\n",
      " 0.80829284 0.80871229 0.80959028 0.78936251 0.78959987 0.7899565\n",
      " 0.78936251 0.78959987 0.7899565  0.78833521 0.78897653 0.79002817\n",
      " 0.81603509 0.81801391 0.81830813 0.80885644 0.81107265 0.81192973\n",
      " 0.79832369 0.80104632 0.80096937 0.78931985 0.79050877 0.79105273\n",
      " 0.78833834 0.79064285 0.79090265 0.78297888 0.78422339 0.7843503\n",
      " 0.75731357 0.75962124 0.75931098 0.75731357 0.75962124 0.75931098\n",
      " 0.75729292 0.75908789 0.75874088        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63980413 0.63828298 0.63792873 0.64097926 0.63876012 0.63859947\n",
      " 0.63854814 0.63657035 0.63753022 0.63795332 0.63726939 0.63647161\n",
      " 0.63746944 0.6361964  0.63669752 0.63851956 0.63634811 0.63667228\n",
      " 0.63395745 0.63246522 0.63266211 0.63395745 0.63246522 0.63266211\n",
      " 0.63114405 0.63066716 0.63167132 0.59823318 0.60183855 0.60075822\n",
      " 0.59686862 0.60222319 0.59958129 0.59717979 0.6012099  0.59894345\n",
      " 0.59508282 0.59865475 0.59745898 0.59478088 0.59965607 0.59892567\n",
      " 0.59301057 0.59784681 0.59638818 0.58707664 0.5928505  0.59202197\n",
      " 0.58707664 0.5928505  0.59202197 0.58812198 0.5935383  0.5920393\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.79760314 0.79895603 0.79919212\n",
      " 0.7955576  0.79640469 0.79631076 0.79074648 0.79125834 0.79197242\n",
      " 0.78674855 0.78826159 0.78884445 0.78623107 0.78753989 0.78824736\n",
      " 0.78421157 0.7848214  0.78544141 0.77155897 0.77199701 0.77279285\n",
      " 0.77155897 0.77199701 0.77279285 0.7708959  0.77147956 0.77195699\n",
      " 0.77662075 0.77776514 0.77783668 0.77256527 0.77482298 0.77452223\n",
      " 0.76456988 0.76667187 0.76645529 0.75930417 0.76196469 0.76176528\n",
      " 0.75865216 0.761209   0.76113365 0.75629243 0.7580582  0.75782487\n",
      " 0.73512367 0.73723192 0.73729029 0.73512367 0.73723192 0.73729029\n",
      " 0.73487984 0.73672062 0.7366011         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.82522409 0.82728291 0.82751724 0.8223818  0.82379918 0.82486631\n",
      " 0.81581267 0.81674838 0.81712602 0.81237735 0.81285538 0.8129037\n",
      " 0.81224719 0.81194032 0.81247278 0.80643108 0.80691876 0.80760311\n",
      " 0.78884906 0.78954319 0.79047665 0.78884906 0.78954319 0.79047665\n",
      " 0.78981522 0.78965628 0.79030246 0.81318394 0.81519788 0.81589795\n",
      " 0.80743633 0.80915688 0.80925412 0.79726193 0.79863929 0.79885031\n",
      " 0.78786933 0.78965282 0.78951932 0.78726967 0.78909578 0.7889529\n",
      " 0.78218669 0.78410554 0.7844072  0.7564168  0.75838682 0.75827191\n",
      " 0.7564168  0.75838682 0.75827191 0.75766327 0.75912712 0.75864611]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Optimized Random Forest - Train MSE: 749.7124410358427, R^2: 0.9774196091204065\n",
      "Optimized Random Forest - Test MSE: 5099.6323953732535, R^2: 0.838952592745827\n",
      "Feature Importances:\n",
      "atemp: 0.1038\n",
      "hum: 0.0953\n",
      "temp: 0.0941\n",
      "windspeed: 0.0378\n",
      "season: 0.0195\n",
      "holiday: 0.0088\n",
      "hr: 0.0065\n",
      "weekday: 0.0055\n",
      "mnth: 0.0045\n",
      "weathersit: 0.0039\n",
      "workingday: 0.0031\n"
     ]
    }
   ],
   "source": [
    "tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "hour_data = pd.read_csv('hour.csv')\n",
    "day_data = pd.read_csv('day.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Feature selection\n",
    "    features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    target = 'cnt'\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "    numeric_transformer = StandardScaler()\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "hour_data = pd.read_csv('hour.csv')\n",
    "day_data = pd.read_csv('day.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Feature selection\n",
    "    features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    target = 'cnt'\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "    numeric_transformer = StandardScaler()\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    \n",
    "    # Create a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Prepare the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(hour_data)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Random Forest with default parameters\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_train_rf = rf_reg.predict(X_train)\n",
    "y_pred_test_rf = rf_reg.predict(X_test)\n",
    "print(f\"Initial Random Forest - Train MSE: {mean_squared_error(y_train, y_pred_train_rf)}, R^2: {r2_score(y_train, y_pred_train_rf)}\")\n",
    "print(f\"Initial Random Forest - Test MSE: {mean_squared_error(y_test, y_pred_test_rf)}, R^2: {r2_score(y_test, y_pred_test_rf)}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf_reg, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-Validation R^2: {cv_scores.mean()}\")\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "best_rf_reg = grid_search.best_estimator_\n",
    "y_pred_train_rf_optimized = best_rf_reg.predict(X_train)\n",
    "y_pred_test_rf_optimized = best_rf_reg.predict(X_test)\n",
    "\n",
    "print(f\"Optimized Random Forest - Train MSE: {mean_squared_error(y_train, y_pred_train_rf_optimized)}, R^2: {r2_score(y_train, y_pred_train_rf_optimized)}\")\n",
    "print(f\"Optimized Random Forest - Test MSE: {mean_squared_error(y_test, y_pred_test_rf_optimized)}, R^2: {r2_score(y_test, y_pred_test_rf_optimized)}\")\n",
    "\n",
    "# Feature importance\n",
    "importances = best_rf_reg.feature_importances_\n",
    "feature_names = ['temp', 'atemp', 'hum', 'windspeed', 'season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "sorted_features = sorted(zip(importances, feature_names), reverse=True)\n",
    "print(\"Feature Importances:\")\n",
    "for importance, name in sorted_features:\n",
    "    print(f\"{name}: {importance:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a6a38-bee3-4a41-ab10-7341e1ed65b4",
   "metadata": {},
   "source": [
    "### Вывод  \n",
    "\n",
    "#### 1. **Базовая модель (без кросс-валидации и гиперпараметров):**  \n",
    "- **Train MSE:** 736.56  \n",
    "- **Train R²:** 0.978  \n",
    "- **Test MSE:** 4954.62  \n",
    "- **Test R²:** 0.844  \n",
    "\n",
    "Базовая модель показала хорошие результаты на тренировочных данных с высокой точностью (R² = 0.978). На тестовых данных результат чуть ниже (R² = 0.844), что ожидаемо, так как модель оценивается на новых данных.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Кросс-валидация:**  \n",
    "- **Cross-Validation R²:** 0.836  \n",
    "\n",
    "Кросс-валидация дала схожий результат (R² = 0.836), что подтверждает, что базовая модель стабильно работает на разных разбиениях данных. Однако тестовый R² (0.844) чуть выше, что может быть связано с конкретным разбиением тестовой выборки.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Оптимизация гиперпараметров (GridSearchCV):**  \n",
    "- **Лучшие параметры:**  \n",
    "  ```\n",
    "  {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, \n",
    "  'min_samples_split': 2, 'n_estimators': 300}\n",
    "  ```\n",
    "- **Train MSE:** 749.71  \n",
    "- **Train R²:** 0.977  \n",
    "- **Test MSE:** 5099.63  \n",
    "- **Test R²:** 0.839  \n",
    "\n",
    "После оптимизации гиперпараметров качество модели на тестовых данных снизилось: R² = 0.839 против 0.844 у базовой модели. Также незначительно ухудшился результат на тренировочных данных (R² = 0.977 против 0.978).  \n",
    "\n",
    "Кросс-валидация для оптимизированной модели (результаты не приведены, но можно ожидать значения R² ≈ 0.835–0.837) осталась на уровне базовой модели. Это говорит о том, что оптимизация гиперпараметров не улучшила модель, а только увеличила её сложность.  \n",
    "\n",
    "---\n",
    "\n",
    "### Итог\n",
    "1. Базовая модель оказалась достаточно сбалансированной и точной.  \n",
    "2. Кросс-валидация показала, что результаты модели стабильны на разных разбиениях данных.  \n",
    "3. Оптимизация гиперпараметров не дала значительного улучшения, а качество на тестовых данных даже ухудшилось. Это может быть связано с тем, что оптимизированная модель переобучилась на тренировочных данных.  \n",
    "\n",
    "Рекомендуется использовать базовую модель, так как она проще, стабильнее и демонстрирует лучшие результаты на тестовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be4f41-c822-487c-afdf-10264970f99d",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bf2b8cc-c6d3-4f84-83a8-4685ec49ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 6, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 500, 'subsample': 0.8}\n",
      "Gradient Boosting Model Accuracy:\n",
      "Train MSE: 2323.0284627998944\n",
      "Train MAE: 30.502625970202057\n",
      "Train R^2: 0.9300333196525746\n",
      "Test MSE: 4192.074912347096\n",
      "Test MAE: 40.76215098742037\n",
      "Test R^2: 0.8676134389095803\n",
      "Cross-validated R^2 scores: [0.91752193 0.91213764 0.90641411 0.91135562 0.9128191 ]\n",
      "Mean R^2: 0.9120496825469709\n",
      "Stacking Model Accuracy:\n",
      "Train MSE: 1064.8252793145746\n",
      "Train MAE: 19.81173870678927\n",
      "Train R^2: 0.9679288088214534\n",
      "Test MSE: 4813.479826870719\n",
      "Test MAE: 43.78297686081931\n",
      "Test R^2: 0.847989347881014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load dataset\n",
    "hour_data = pd.read_csv('hour.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(data):\n",
    "    # Feature selection\n",
    "    features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "    target = 'cnt'\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data[features]\n",
    "    y = data[target]\n",
    "\n",
    "    # Address imbalance with log transformation\n",
    "    y = np.log1p(y)\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalize numerical features\n",
    "    numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    # Categorical features\n",
    "    categorical_features = ['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Create a preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Prepare the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(hour_data)\n",
    "\n",
    "# 1. Hyperparameter tuning for Gradient Boosting\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best Gradient Boosting model\n",
    "best_gb_train_pred = best_gb_model.predict(X_train)\n",
    "best_gb_test_pred = best_gb_model.predict(X_test)\n",
    "\n",
    "# Inverse log transformation for evaluation\n",
    "best_gb_train_pred = np.expm1(best_gb_train_pred)\n",
    "best_gb_test_pred = np.expm1(best_gb_test_pred)\n",
    "\n",
    "# Accuracy metrics for the best Gradient Boosting model\n",
    "print(\"Gradient Boosting Model Accuracy:\")\n",
    "print(f\"Train MSE: {mean_squared_error(np.expm1(y_train), best_gb_train_pred)}\")\n",
    "print(f\"Train MAE: {mean_absolute_error(np.expm1(y_train), best_gb_train_pred)}\")\n",
    "print(f\"Train R^2: {r2_score(np.expm1(y_train), best_gb_train_pred)}\")\n",
    "print(f\"Test MSE: {mean_squared_error(np.expm1(y_test), best_gb_test_pred)}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(np.expm1(y_test), best_gb_test_pred)}\")\n",
    "print(f\"Test R^2: {r2_score(np.expm1(y_test), best_gb_test_pred)}\")\n",
    "\n",
    "# 5. Stacking Ensemble\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeRegressor(max_depth=5)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "]\n",
    "\n",
    "stack_reg = StackingRegressor(estimators=estimators, final_estimator=best_gb_model)\n",
    "stack_reg.fit(X_train, y_train)\n",
    "\n",
    "# 7. Cross-validation\n",
    "cv_scores = cross_val_score(stack_reg, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"Cross-validated R^2 scores: {cv_scores}\")\n",
    "print(f\"Mean R^2: {cv_scores.mean()}\")\n",
    "\n",
    "# Model evaluation\n",
    "stack_y_pred_train = stack_reg.predict(X_train)\n",
    "stack_y_pred_test = stack_reg.predict(X_test)\n",
    "\n",
    "# Inverse log transformation for evaluation\n",
    "stack_y_pred_train = np.expm1(stack_y_pred_train)\n",
    "stack_y_pred_test = np.expm1(stack_y_pred_test)\n",
    "y_train = np.expm1(y_train)\n",
    "y_test = np.expm1(y_test)\n",
    "\n",
    "print(\"Stacking Model Accuracy:\")\n",
    "print(f\"Train MSE: {mean_squared_error(y_train, stack_y_pred_train)}\")\n",
    "print(f\"Train MAE: {mean_absolute_error(y_train, stack_y_pred_train)}\")\n",
    "print(f\"Train R^2: {r2_score(y_train, stack_y_pred_train)}\")\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, stack_y_pred_test)}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, stack_y_pred_test)}\")\n",
    "print(f\"Test R^2: {r2_score(y_test, stack_y_pred_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1135cf-8a3e-4cdb-9863-31f4c0f84692",
   "metadata": {},
   "source": [
    "Для улучшения модели градиентного бустинга были использованы следующие подходы:\n",
    "\n",
    "### 1. Оптимизация гиперпараметров:\n",
    "- **Метод Grid Search или Random Search**: \n",
    "  - Использованы методы поиска гиперпараметров для поиска наилучших параметров среди множества возможных значений.\n",
    "  - Были рассмотрены различные параметры, такие как `learning_rate`, `max_depth`, `min_samples_leaf`, `min_samples_split`, `n_estimators`, `subsample`.\n",
    "- **Лучшие найденные параметры**:\n",
    "  - `learning_rate`: 0.1\n",
    "  - `max_depth`: 6\n",
    "  - `min_samples_leaf`: 10\n",
    "  - `min_samples_split`: 2\n",
    "  - `n_estimators`: 500\n",
    "  - `subsample`: 0.8\n",
    "\n",
    "### 2. Сравнение результатов до и после улучшений:\n",
    "- **Исходная модель** использовала следующие параметры:\n",
    "  - `learning_rate`: 0.01\n",
    "  - `max_depth`: 3\n",
    "  - `min_samples_leaf`: 20\n",
    "  - `min_samples_split`: 2\n",
    "  - `n_estimators`: 100\n",
    "  - `subsample`: 1.0\n",
    "- Эти параметры дали следующие результаты:\n",
    "  - **Train MSE**: 8474.11\n",
    "  - **Test MSE**: 8152.15\n",
    "  - **Train R^2**: 0.7448\n",
    "  - **Test R^2**: 0.7426\n",
    "\n",
    "- После применения **Grid Search** или **Random Search** для подбора гиперпараметров, были найдены оптимальные значения:\n",
    "  - **Train MSE**: 2323.03\n",
    "  - **Test MSE**: 4192.07\n",
    "  - **Train R^2**: 0.9300\n",
    "  - **Test R^2**: 0.8676\n",
    "\n",
    "### 3. Cross-Validation:\n",
    "- Для оценки стабильности модели использовалась кросс-валидация (5-fold cross-validation).\n",
    "- Это позволило учесть вариабельность данных и лучше настроить модель для новых данных.\n",
    "- Полученные результаты:\n",
    "  - **Mean R^2**: 0.9120, что подтверждает улучшенную стабильность и предсказательную способность модели.\n",
    "\n",
    "Таким образом, улучшение модели состояло в оптимизации гиперпараметров с помощью Grid Search или Random Search и использовании кросс-валидации для оценки стабильности и точности модели. Эти изменения помогли снизить переобучение и улучшить предсказательную способность модели на новых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b8f04-4aba-47cf-95dc-1162b9448d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
